# 命令行界面参考

`mdl-toolkit`提供了以下子命令：

## `mdl-toolkit convert-dataset` --- 转换数据集

`mdl-toolkit convert-dataset`命令用于对数据集进行转换。该过程会将 CSV 格式的数据集转换为包含训练所需参数的 Huggingface Datasets 格式，添加必要的特殊字符，进行分词，并生成训练标签。

如果在`mdl-toolkit train`命令中指定 CSV 格式的数据集，则该数据集会在训练前进行转换。在这种情况下，`mdl-toolkit convert-dataset`的所有选项（除输入和输出部分）也适用于`mdl-toolkit train`以控制转换过程。

`mdl-toolkit inference`命令使用类似的输入格式进行推理。`mdl-toolkit convert-dataset`的所有选项（除输入和输出部分）也适用于`mdl-toolkit inference`，并且应该在训练时和推理时保持一致。

**通用选项**

* `--model-name`：**默认值：`mispeech/midashenglm-7b`** 对转换和训练可选，模型的 Huggingface 名称或本地路径。
* `--from-modelscope`：**默认值：`false`** 是否从 ModelScope 加载模型。如果设置为`true`，将从 ModelScope 加载模型，否则将从 Huggingface 加载模型。从 ModelScope 加载模型需要启用`modelscope`可选功能，参见[安装文档](installation.md)。
* `--tokenizing-batch-size`：**默认值：`8`** 分词时使用的批量大小。
* `--num-workers`：**默认值：（动态）** 处理数据时使用的工作进程数量。默认使用可用 CPU 核心数的一半，最大不超过 32。受实现影响，该数值仅控制一部分转换流程的并行化。

**数据集选项**

* `--system-prompt`：**默认值：`null`** 默认系统提示词，用于指导模型的行为。如果数据集中提供了`system_prompt`列，则该列的非空值将覆盖该默认值。
* `--user-prompt`：**默认值：`null`** 默认用户提示词，用于指导模型的行为。如果数据集中提供了`user_prompt`列，则该列的非空值将覆盖该默认值。
* `--base-dir`：**默认值：`null`** 数据集的根目录。如果指定，数据集中的相对路径将相对于该目录进行解析。如果未指定，则相对路径将相对于命令的当前工作目录进行解析。

**输入和输出**

* `INPUT`：**必需，位置参数** 输入数据集的 CSV 文件路径。
* `--output`：**必需** 输出数据集的保存路径。现有的文件将被覆盖。

## `mdl-toolkit train` --- 使用数据集对模型进行训练

`mdl-toolkit train`命令用于对模型进行训练。训练过程会加载预训练模型，在指定的数据集上进行微调，并保存训练后的模型。如果配置了评估数据集，则会在训练过程中进行评估并报告评估集上的损失。默认配置下，训练过程会自动保存检查点，以便在训练中断时可以恢复。

`mdl-toolkit train`命令可以使用 CSV 格式的数据集或转换后的数据集。如果指定了 CSV 格式的数据集，则该数据集会在训练前进行转换。在这种情况下，`mdl-toolkit convert-dataset`的所有选项（除输入和输出部分）也可以用于`mdl-toolkit train`以控制转换过程。如果指定了转换后的数据集，则转换选项将被忽略。

**训练选项**

* `--train-dataset`：**必需** 训练数据集的路径。
* `--lr`：**默认值：`1e-4`** 学习率，控制优化器更新参数的速率。
* `--lora-rank`：**默认值：`32`** LoRA 的秩，控制 LoRA 适配器的复杂度。较高的秩可以捕捉更多的特征，但也会增加计算和存储开销，并增加过拟合的风险。对于简单的任务，建议尝试 8~16；对于复杂任务，可以尝试 32 或更高，一般不超过 128。
* `--lora-alpha`：**默认值：`32`** LoRA 中的 alpha 参数，控制 LoRA 适配器的缩放。
* `--lora-dropout`：**默认值：`0`** LoRA 适配器的 dropout 率。
* `--train-target`：**默认值：`["encoder", "projector", "decoder"]`** 训练的目标模块，可以指定`encoder`、`projector`、`decoder`、`embed_tokens`或`lm_head`，分别训练音频编码器、音频投影层、文本解码器、词嵌入层和输出头。可以多次使用以指定多个模块。如果词嵌入层和输出头被指定，将会被完整训练。
* `--num-epochs`：**默认值：`1`** 训练的总轮数。对于 LLM 而言，通常训练 1~3 个 epoch 就足够了。更大的 epoch 数量通常不会显著提高性能，并可能导致过拟合。可以设置为浮点数以进行部分 epoch 训练。
* `--warmup-steps`：**默认值：`0`** 学习率预热的步数。预热将在训练初期逐步增加学习率，可能会提高训练稳定性。

**显存选项**

* `--batch-size`：**默认值：`8`** 每个训练步骤中每个 GPU 设备处理的样本数量。较大的批量大小可能会提高训练速度并增加模型的稳定性，但也会增加内存使用量。如果同时设置梯度累积或使用多个 GPU 并行，实际的有效批次大小为：`batch_size * gradient_accumulation_steps * num_gpus`。LLM 微调通常对批量大小不敏感，因此一般根据显存大小进行调整。
* `--gradient-accumulation-steps`：**默认值：`1`** 梯度累积的步数。在更新模型参数之前，累积多个训练步骤的梯度。当批量大小受限于显存时，可以通过增加梯度累积步数来模拟更大的批量大小。
* `--gradient-checkpointing`：**默认值：`true`** 是否启用梯度检查点。启用后，可以大幅节省显存，但会增加少量计算开销。
* `--bf16`：**默认值：（动态）** 是否使用 bfloat16 加载模型权重，当 CUDA 可用并且支持 bfloat16 时默认启用，否则默认禁用。启用后，相较于 float32，模型的内存占用将大幅减少，并且计算速度可能会有所提高。使用 bfloat16 时，模型的精度可能会略有下降，但通常不会影响训练效果。
* `--quantization`：**默认值：`null`** 对模型权重进行量化，可以选择`8bit`或`4bit`。启用后，可以进一步减少模型的内存占用，但可能产生计算开销，并对模型性能产生少量影响。要进行量化或加载量化模型，需要启用`quantization`可选功能，参见[安装文档](installation.md)。

**评估选项**

* `--eval-dataset`：**可选** 评估数据集的路径。如果指定，将在训练过程中进行评估并报告评估集上的损失。如果未指定，则不进行评估并忽略其他评估选项。
* `--eval-steps`：**默认值：`500`** 每隔多少步进行一次评估。评估会产生一定开销，因此建议根据实际情况调整评估频率。
* `--eval-batch-size`：**默认值：`null`** 评估时每个 GPU 设备处理的样本数量。如果未指定，将使用训练批量大小。由于评估时仅运行前向传播且无需保存激活，因此一般可以使用更大的批量大小以提高评估速度。
* `--eval-accumulation-steps`：**默认值：`null`** 评估时累积结果的步数。指定该参数可以在评估时累积多个步骤的结果，从而减少传输开销。如果未指定，则不进行累积。
* `--report-to`：**默认值：`[]`，可多次指定** 指定将训练和评估指标报告到哪些平台。可以多次使用以指定多个平台。支持的平台参见[`transformers`文档](https://huggingface.co/docs/transformers/v4.53.3/en/main_classes/trainer#transformers.TrainingArguments.report_to)。

**检查点与输出选项**

* `--output`：**必需** 输出目录的路径。训练过程中的检查点和训练结果将保存在该目录中。
* `--resume-from-checkpoint`：**默认值：`null`** 从指定的检查点恢复训练。如果设置为`null`或`false`，则从头开始训练。如果设置为`true`，则从最后一个检查点恢复训练。如果设置为具体的检查点路径，则从路径指定的检查点恢复训练。
* `--save-steps`：**默认值：`500`** 每隔多少步保存一次模型检查点。如果设置为`>=1`的整数，将每隔该步数保存一次检查点。如果设置为`[0, 1)`间的浮点数，将每隔该比例的 epoch 保存一次检查点。
* `--save-total-limit`：**默认值：`null`** 最多保存多少个模型检查点。如果设置为`null`，则不限制保存的检查点数量。如果设置为正整数，将在超过该数量时删除最旧的检查点。
* `--merge-lora`：**默认值：`true`** 输出训练结果前，是否需要合并 LoRA 适配器。如果设置，将在输出模型前合并 LoRA 适配器并输出完整模型，输出格式与原始模型相同，无需修改代码即可使用，但会增加模型占用的空间。如果不设置，则只输出 LoRA 适配器和被修改的模型权重。

## `mdl-toolkit inference` --- 使用模型进行推理

`mdl-toolkit inference`命令提供了一个简单的接口，用于在给定输入上运行模型并生成输出。推理时，输入数据的系统提示和用户提示应与训练时保持一致，以确保模型输出的内容符合预期，模型输出的内容应当与训练数据的格式一致。

`mdl-toolkit inference`旨在快速测试训练后的模型，该命令并未针对性能和灵活性进行优化。要在生产环境中使用模型，请考虑`vllm`和其他专用推理框架。

`mdl-toolkit inference`命令使用与训练输入类似的格式进行推理。`mdl-toolkit convert-dataset`的所有选项（除输入和输出部分）也适用于`mdl-toolkit inference`，并且应该在训练时和推理时保持一致以确保模型输出的内容符合预期。

**推理选项**

* `INPUT`：**必需，位置参数** 输入数据集的 CSV 文件路径。
* `--output`：**必需** 输出数据集的保存路径。现有的文件将被覆盖。
* `--model-name`：**必需** 对于推理必需，模型的 Huggingface 名称或本地路径。
* `--batch-size`：**默认值：`32`** 每个推理步骤中每个 GPU 设备处理的样本数量。较大的批量大小可能会提高推理速度，但也会增加内存使用量。
* `--max-length`：**默认值：`128`** 序列的最大长度，包括输入、输出和所有特殊标记。如果输出序列的长度超过该长度，输出将被截断。如果输入序列的长度超过该值，将导致错误。
